"""
PH√ÇN T√çCH CHUY√äN S√ÇU D·ªÆ LI·ªÜU SQLiV3.csv
========================================

Script n√†y gi√∫p b·∫°n hi·ªÉu ƒë·∫ßy ƒë·ªß v·ªÅ:
1. C·∫•u tr√∫c d·ªØ li·ªáu
2. ƒê·∫∑c tr∆∞ng c·ªßa SQL injection
3. Ph√¢n ph·ªëi v√† patterns
4. C√°c lo·∫°i t·∫•n c√¥ng
5. ƒê·∫∑c ƒëi·ªÉm ng√¥n ng·ªØ h·ªçc (linguistic features)

Tham kh·∫£o: Paper Section 2.1 (Datasets)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from pathlib import Path

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# ==============================================================================
# PH·∫¶N 1: LOAD V√Ä KH·∫¢O S√ÅT BAN ƒê·∫¶U
# ==============================================================================

def initial_exploration(file_path):
    """
    B∆∞·ªõc 1: Kh·∫£o s√°t c∆° b·∫£n v·ªÅ dataset
    
    C√¢u h·ªèi:
    - Dataset c√≥ bao nhi√™u d√≤ng, c·ªôt?
    - C√≥ missing values kh√¥ng?
    - C·ªôt n√†o l√† input, c·ªôt n√†o l√† output?
    - Ki·ªÉu d·ªØ li·ªáu ra sao?
    """
    print("=" * 80)
    print("PH·∫¶N 1: KH·∫¢O S√ÅT C∆† B·∫¢N DATASET")
    print("=" * 80)
    
    # Load data
    df = pd.read_csv(file_path)
    
    print(f"\nüìä K√≠ch th∆∞·ªõc dataset:")
    print(f"   - S·ªë d√≤ng (samples): {len(df):,}")
    print(f"   - S·ªë c·ªôt (features): {df.shape[1]}")
    
    print(f"\nüìã C√°c c·ªôt trong dataset:")
    for i, col in enumerate(df.columns, 1):
        print(f"   {i}. {col} (dtype: {df[col].dtype})")
    
    print(f"\nüîç Th√¥ng tin chi ti·∫øt:")
    print(df.info())
    
    print(f"\n‚ùì Missing values:")
    missing = df.isnull().sum()
    if missing.sum() == 0:
        print("   ‚úì Kh√¥ng c√≥ missing values")
    else:
        print(missing[missing > 0])
    
    print(f"\nüëÄ 5 d√≤ng ƒë·∫ßu ti√™n:")
    print(df.head())
    
    return df


# ==============================================================================
# PH·∫¶N 2: PH√ÇN T√çCH NH√ÉN (LABELS)
# ==============================================================================

def analyze_labels(df):
    """
    B∆∞·ªõc 2: Ph√¢n t√≠ch c·ªôt Label
    
    C√¢u h·ªèi:
    - C√≥ bao nhi√™u class?
    - Ph√¢n b·ªë m·ªói class nh∆∞ th·∫ø n√†o?
    - Dataset c√≥ imbalanced kh√¥ng?
    """
    print("\n" + "=" * 80)
    print("PH·∫¶N 2: PH√ÇN T√çCH NH√ÉN (LABELS)")
    print("=" * 80)
    
    # Ensure Label column exists
    if 'Label' not in df.columns:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·ªôt 'Label'")
        return
    
    print(f"\nüìä Ph√¢n b·ªë nh√£n:")
    label_counts = df['Label'].value_counts().sort_index()
    
    print(f"\nüî¢ S·ªë l∆∞·ª£ng classes: {len(label_counts)}")
    
    # If more than 2 classes, need to clean first
    if len(label_counts) > 2:
        print(f"\n‚ö†Ô∏è  C·∫¢NH B√ÅO: Dataset c√≥ {len(label_counts)} classes!")
        print("   Paper ch·ªâ s·ª≠ d·ª•ng 2 classes: 0 (Benign) v√† 1 (Malicious)")
        print("   ƒêang hi·ªÉn th·ªã t·∫•t c·∫£ classes hi·ªán t·∫°i:")
        print()
        
        for label, count in label_counts.items():
            percentage = (count / len(df)) * 100
            print(f"   Label {label}: {count:,} ({percentage:.2f}%)")
        
        print("\nüí° G·ª¢I √ù: B·∫°n c·∫ßn clean dataset ƒë·ªÉ ch·ªâ gi·ªØ l·∫°i Label 0 v√† 1")
        print("   Th√™m d√≤ng sau v√†o code:")
        print("   df = df[df['Label'].isin([0, 1])]")
        
        # Check balance
        ratio = label_counts.max() / label_counts.min()
        print(f"\n‚öñÔ∏è  Class balance ratio: {ratio:.2f}")
        print("   (ratio gi·ªØa class l·ªõn nh·∫•t v√† nh·ªè nh·∫•t)")
        
        # Simple bar chart for all classes
        fig, ax = plt.subplots(figsize=(12, 6))
        
        labels_list = [f"Label {l}" for l in label_counts.index]
        colors_list = plt.cm.Set3(np.linspace(0, 1, len(label_counts)))
        
        bars = ax.bar(labels_list, label_counts.values, color=colors_list, 
                     alpha=0.7, edgecolor='black')
        ax.set_ylabel('Number of Samples')
        ax.set_title('Class Distribution (All Labels)')
        ax.grid(axis='y', alpha=0.3)
        
        # Add value labels
        for i, (bar, count) in enumerate(zip(bars, label_counts.values)):
            percentage = (count / len(df)) * 100
            ax.text(bar.get_x() + bar.get_width()/2, count + len(df)*0.01, 
                   f'{count:,}\n({percentage:.1f}%)', 
                   ha='center', fontweight='bold', fontsize=9)
        
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig('label_distribution.png', dpi=300, bbox_inches='tight')
        print("\n‚úì Saved: label_distribution.png")
        plt.show()
        
        return
    
    # Original code for binary classification
    for label, count in label_counts.items():
        percentage = (count / len(df)) * 100
        label_name = "Benign (Normal)" if label == 0 else "Malicious (SQL Injection)"
        print(f"   Label {label} - {label_name}:")
        print(f"      S·ªë l∆∞·ª£ng: {count:,} ({percentage:.2f}%)")
    
    # Check balance
    ratio = label_counts.max() / label_counts.min()
    print(f"\n‚öñÔ∏è  Class balance ratio: {ratio:.2f}")
    if ratio < 1.5:
        print("   ‚úì Dataset kh√° c√¢n b·∫±ng (balanced)")
    elif ratio < 3:
        print("   ‚ö†Ô∏è  Dataset h∆°i m·∫•t c√¢n b·∫±ng (slightly imbalanced)")
    else:
        print("   ‚ùå Dataset m·∫•t c√¢n b·∫±ng nghi√™m tr·ªçng (highly imbalanced)")
    
    return label_counts


# ==============================================================================
# PH·∫¶N 3: PH√ÇN T√çCH TRUY V·∫§N (QUERIES)
# ==============================================================================

def analyze_queries(df):
    """
    B∆∞·ªõc 3: Ph√¢n t√≠ch c·ªôt Sentence (SQL queries)
    
    C√¢u h·ªèi:
    - ƒê·ªô d√†i truy v·∫•n nh∆∞ th·∫ø n√†o?
    - Benign vs Malicious c√≥ kh√°c bi·ªát v·ªÅ ƒë·ªô d√†i?
    - T·ª´ n√†o xu·∫•t hi·ªán nhi·ªÅu nh·∫•t?
    """
    print("\n" + "=" * 80)
    print("PH·∫¶N 3: PH√ÇN T√çCH TRUY V·∫§N SQL")
    print("=" * 80)
    
    if 'Sentence' not in df.columns:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·ªôt 'Sentence'")
        return
    
    # Calculate query lengths
    df['query_length'] = df['Sentence'].str.len()
    df['word_count'] = df['Sentence'].str.split().str.len()
    
    print(f"\nüìè Th·ªëng k√™ ƒë·ªô d√†i truy v·∫•n (k√Ω t·ª±):")
    print(df['query_length'].describe())
    
    print(f"\nüìù Th·ªëng k√™ s·ªë t·ª´ trong truy v·∫•n:")
    print(df['word_count'].describe())
    
    # Compare benign vs malicious
    print(f"\nüîç So s√°nh Benign vs Malicious:")
    
    if 'Label' in df.columns:
        for label in sorted(df['Label'].unique()):
            label_name = "Benign" if label == 0 else "Malicious"
            subset = df[df['Label'] == label]
            
            print(f"\n   {label_name} (Label={label}):")
            print(f"      ƒê·ªô d√†i TB: {subset['query_length'].mean():.1f} k√Ω t·ª±")
            print(f"      ƒê·ªô d√†i Min: {subset['query_length'].min()}")
            print(f"      ƒê·ªô d√†i Max: {subset['query_length'].max()}")
            print(f"      S·ªë t·ª´ TB: {subset['word_count'].mean():.1f} t·ª´")
    
    return df


# ==============================================================================
# PH·∫¶N 4: PH√ÅT HI·ªÜN C√ÅC LO·∫†I T·∫§N C√îNG SQL INJECTION
# ==============================================================================

def detect_attack_types(df):
    """
    B∆∞·ªõc 4: Ph√¢n lo·∫°i c√°c lo·∫°i t·∫•n c√¥ng SQL injection
    
    C√°c lo·∫°i ch√≠nh (theo paper):
    1. UNION-based: UNION SELECT
    2. Boolean-based (Tautology): OR 1=1, AND 1=1
    3. Time-based blind: SLEEP(), WAITFOR DELAY
    4. Error-based: CAST, CONVERT, Syntax errors
    5. Comment-based: --, #, /* */
    6. Stacked queries: ; (multiple statements)
    """
    print("\n" + "=" * 80)
    print("PH·∫¶N 4: PH√ÅT HI·ªÜN C√ÅC LO·∫†I T·∫§N C√îNG SQL INJECTION")
    print("=" * 80)
    
    if 'Label' not in df.columns or 'Sentence' not in df.columns:
        print("‚ö†Ô∏è  Thi·∫øu c·ªôt c·∫ßn thi·∫øt")
        return
    
    # Only analyze malicious queries
    malicious = df[df['Label'] == 1]['Sentence']
    
    print(f"\nüîç Ph√¢n t√≠ch {len(malicious):,} malicious queries...")
    
    # Define attack patterns
    attack_patterns = {
        'UNION-based': [
            r'\bUNION\b.*\bSELECT\b',
        ],
        'Boolean-based (Tautology)': [
            r'\bOR\s+[\d\'\"]+\s*=\s*[\d\'\"]+',  # OR 1=1, OR '1'='1'
            r'\bAND\s+[\d\'\"]+\s*=\s*[\d\'\"]+',  # AND 1=1
            r'\bOR\s+1\s*=\s*1\b',
        ],
        'Time-based Blind': [
            r'\bSLEEP\s*\(',
            r'\bWAITFOR\s+DELAY\b',
            r'\bBENCHMARK\s*\(',
            r'\bpg_sleep\s*\(',
        ],
        'Error-based': [
            r'\bCONVERT\s*\(',
            r'\bCAST\s*\(',
            r'\bEXTRACTVALUE\s*\(',
            r'\bUPDATEXML\s*\(',
        ],
        'Comment-based': [
            r'--',
            r'#',
            r'/\*.*?\*/',
        ],
        'Stacked Queries': [
            r';\s*\w+',  # ; followed by another statement
        ],
        'String Manipulation': [
            r'CONCAT\s*\(',
            r'\|\|',  # String concatenation
            r'\+\s*[\'"]',  # String concatenation with +
        ],
    }
    
    # Detect patterns
    attack_counts = {}
    
    for attack_type, patterns in attack_patterns.items():
        count = 0
        for pattern in patterns:
            count += malicious.str.contains(pattern, case=False, regex=True, na=False).sum()
        attack_counts[attack_type] = count
    
    # Display results
    print(f"\nüìä Ph√¢n lo·∫°i c√°c lo·∫°i t·∫•n c√¥ng:")
    total_detected = sum(attack_counts.values())
    
    for attack_type, count in sorted(attack_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(malicious)) * 100
        print(f"   {attack_type}:")
        print(f"      S·ªë l∆∞·ª£ng: {count:,} ({percentage:.2f}% of malicious)")
    
    print(f"\n   ‚ö†Ô∏è  L∆∞u √Ω: M·ªôt query c√≥ th·ªÉ thu·ªôc nhi·ªÅu lo·∫°i t·∫•n c√¥ng")
    print(f"   Total detections: {total_detected:,}")
    
    # Show examples for each type
    print(f"\nüìù V√ç D·ª§ C·ª§ TH·ªÇ CHO M·ªñI LO·∫†I:")
    
    for attack_type, patterns in attack_patterns.items():
        print(f"\n{'='*70}")
        print(f"üî¥ {attack_type}")
        print(f"{'='*70}")
        
        # Find examples
        found = False
        for pattern in patterns:
            matches = malicious[malicious.str.contains(pattern, case=False, regex=True, na=False)]
            if len(matches) > 0:
                print(f"\nPattern: {pattern}")
                for i, example in enumerate(matches.head(2), 1):
                    # Truncate long queries
                    display = example[:150] + "..." if len(example) > 150 else example
                    print(f"   Example {i}: {display}")
                found = True
                break
        
        if not found:
            print(f"   (Kh√¥ng t√¨m th·∫•y v√≠ d·ª•)")
    
    return attack_counts, len(malicious)


# ==============================================================================
# PH·∫¶N 5: PH√ÇN T√çCH T·ª™ V·ª∞NG (VOCABULARY)
# ==============================================================================

def analyze_vocabulary(df):
    """
    B∆∞·ªõc 5: Ph√¢n t√≠ch t·ª´ v·ª±ng
    
    C√¢u h·ªèi:
    - T·ª´ n√†o xu·∫•t hi·ªán nhi·ªÅu nh·∫•t?
    - C√≥ s·ª± kh√°c bi·ªát gi·ªØa benign v√† malicious kh√¥ng?
    - Keywords n√†o ƒë·∫∑c tr∆∞ng cho SQL injection?
    """
    print("\n" + "=" * 80)
    print("PH·∫¶N 5: PH√ÇN T√çCH T·ª™ V·ª∞NG")
    print("=" * 80)
    
    if 'Sentence' not in df.columns or 'Label' not in df.columns:
        return
    
    # Tokenize
    def tokenize(text):
        return re.findall(r'\b\w+\b', str(text).lower())
    
    # Separate benign and malicious
    benign_queries = df[df['Label'] == 0]['Sentence']
    malicious_queries = df[df['Label'] == 1]['Sentence']
    
    # Count words
    benign_words = []
    for query in benign_queries:
        benign_words.extend(tokenize(query))
    
    malicious_words = []
    for query in malicious_queries:
        malicious_words.extend(tokenize(query))
    
    benign_counter = Counter(benign_words)
    malicious_counter = Counter(malicious_words)
    
    print(f"\nüìä Th·ªëng k√™ t·ª´ v·ª±ng:")
    print(f"   Benign: {len(benign_counter):,} unique words")
    print(f"   Malicious: {len(malicious_counter):,} unique words")
    
    # Top words in each class
    print(f"\nüîù TOP 20 T·ª™ PH·ªî BI·∫æN TRONG BENIGN:")
    for i, (word, count) in enumerate(benign_counter.most_common(20), 1):
        print(f"   {i:2d}. {word:15s} : {count:6,} l·∫ßn")
    
    print(f"\nüî¥ TOP 20 T·ª™ PH·ªî BI·∫æN TRONG MALICIOUS:")
    for i, (word, count) in enumerate(malicious_counter.most_common(20), 1):
        print(f"   {i:2d}. {word:15s} : {count:6,} l·∫ßn")
    
    # Find discriminative words (ch·ªâ c√≥ trong malicious)
    print(f"\nüéØ T·ª™ ƒê·∫∂C TR∆ØNG CHO SQL INJECTION (ch·ªâ c√≥ trong malicious):")
    
    discriminative = []
    for word, count in malicious_counter.most_common(100):
        if word not in benign_counter and count > 10:
            discriminative.append((word, count))
    
    for i, (word, count) in enumerate(discriminative[:30], 1):
        print(f"   {i:2d}. {word:20s} : {count:5,} l·∫ßn")
    
    return benign_counter, malicious_counter


# ==============================================================================
# PH·∫¶N 6: PH√ÇN T√çCH K√ù T·ª∞ ƒê·∫∂C BI·ªÜT
# ==============================================================================

def analyze_special_characters(df):
    """
    B∆∞·ªõc 6: Ph√¢n t√≠ch k√Ω t·ª± ƒë·∫∑c bi·ªát
    
    SQL injection th∆∞·ªùng ch·ª©a c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát:
    - D·∫•u nh√°y: ', "
    - Comment: --, #, /* */
    - Logic: =, <, >, !=
    - Semicolon: ;
    """
    print("\n" + "=" * 80)
    print("PH·∫¶N 6: PH√ÇN T√çCH K√ù T·ª∞ ƒê·∫∂C BI·ªÜT")
    print("=" * 80)
    
    if 'Sentence' not in df.columns or 'Label' not in df.columns:
        return
    
    special_chars = {
        "Single Quote (')": "'",
        "Double Quote (\")": '"',
        "Semicolon (;)": ";",
        "Double Dash (--)": "--",
        "Hash (#)": "#",
        "Percent (%)": "%",
        "Asterisk (*)": "*",
        "Equals (=)": "=",
        "Pipe (|)": "|",
    }
    
    results = {'Benign': {}, 'Malicious': {}}
    
    for label, label_name in [(0, 'Benign'), (1, 'Malicious')]:
        queries = df[df['Label'] == label]['Sentence']
        
        for char_name, char in special_chars.items():
            count = queries.str.contains(re.escape(char), na=False).sum()
            percentage = (count / len(queries)) * 100
            results[label_name][char_name] = (count, percentage)
    
    # Display
    print(f"\nüìä T·∫ßn su·∫•t k√Ω t·ª± ƒë·∫∑c bi·ªát:")
    print(f"\n{'Character':<20s} | {'Benign':<20s} | {'Malicious':<20s} | Difference")
    print("-" * 80)
    
    for char_name in special_chars.keys():
        benign_count, benign_pct = results['Benign'][char_name]
        mal_count, mal_pct = results['Malicious'][char_name]
        diff = mal_pct - benign_pct
        
        print(f"{char_name:<20s} | {benign_count:5,} ({benign_pct:5.1f}%) | "
              f"{mal_count:5,} ({mal_pct:5.1f}%) | {diff:+6.1f}%")
    
    return results, special_chars


# ==============================================================================
# PH·∫¶N 7: M·∫™U D·ªÆ LI·ªÜU C·ª§ TH·ªÇ
# ==============================================================================

def show_samples(df, n_samples=5):
    """
    B∆∞·ªõc 7: Hi·ªÉn th·ªã m·∫´u c·ª• th·ªÉ
    
    Gi√∫p hi·ªÉu tr·ª±c quan v·ªÅ d·ªØ li·ªáu
    """
    print("\n" + "=" * 80)
    print("PH·∫¶N 7: M·∫™U D·ªÆ LI·ªÜU C·ª§ TH·ªÇ")
    print("=" * 80)
    
    if 'Sentence' not in df.columns or 'Label' not in df.columns:
        return
    
    print(f"\n‚úÖ {n_samples} M·∫™U BENIGN (NORMAL QUERIES):")
    print("=" * 80)
    
    benign_samples = df[df['Label'] == 0].sample(n=n_samples, random_state=42)
    for i, (idx, row) in enumerate(benign_samples.iterrows(), 1):
        print(f"\nSample {i}:")
        print(f"   {row['Sentence']}")
    
    print(f"\n\nüî¥ {n_samples} M·∫™U MALICIOUS (SQL INJECTION):")
    print("=" * 80)
    
    malicious_samples = df[df['Label'] == 1].sample(n=n_samples, random_state=42)
    for i, (idx, row) in enumerate(malicious_samples.iterrows(), 1):
        print(f"\nSample {i}:")
        print(f"   {row['Sentence']}")
        
        # Try to identify attack type
        query = row['Sentence']
        attack_types = []
        
        if re.search(r'\bUNION\b.*\bSELECT\b', query, re.IGNORECASE):
            attack_types.append("UNION-based")
        if re.search(r'\bOR\s+[\d\'\"]+\s*=\s*[\d\'\"]+', query, re.IGNORECASE):
            attack_types.append("Boolean-based")
        if re.search(r'\bSLEEP\s*\(|WAITFOR', query, re.IGNORECASE):
            attack_types.append("Time-based")
        if re.search(r'--|#|/\*', query):
            attack_types.append("Comment-based")
        
        if attack_types:
            print(f"   ‚Üí Detected: {', '.join(attack_types)}")


# ==============================================================================
# MAIN ANALYSIS
# ==============================================================================

def main():
    """
    Ch·∫°y to√†n b·ªô ph√¢n t√≠ch
    """
    print("\n" + "üîç" * 40)
    print("PH√ÇN T√çCH CHUY√äN S√ÇU D·ªÆ LI·ªÜU SQLiV3.csv")
    print("Paper: Enhanced SQL injection detection using chi-square feature selection")
    print("üîç" * 40)
    
    # Check for cleaned file first
    cleaned_file = Path("data/SQLiV3_cleaned.csv")
    original_file = Path("data/SQLiV3.csv")
    
    if cleaned_file.exists():
        print(f"\n‚úì T√¨m th·∫•y file ƒë√£ clean: {cleaned_file}")
        file_path = cleaned_file
    elif original_file.exists():
        print(f"\n‚ö†Ô∏è  Ch·ªâ t√¨m th·∫•y file g·ªëc: {original_file}")
        print(f"   Khuy·∫øn ngh·ªã: ch·∫°y 'python clean_data.py' tr∆∞·ªõc")
        print(f"   ƒêang ph√¢n t√≠ch file g·ªëc...")
        file_path = original_file
    else:
        print(f"\n‚ùå Kh√¥ng t√¨m th·∫•y file SQLiV3.csv ho·∫∑c SQLiV3_cleaned.csv")
        print("   Vui l√≤ng ƒë·∫£m b·∫£o file n·∫±m trong c√πng th∆∞ m·ª•c v·ªõi script n√†y")
        return
    
    # Run all analyses
    df = initial_exploration(file_path)
    
    # Check if dataset needs cleaning
    if 'Label' in df.columns:
        unique_labels = df['Label'].nunique()
        if unique_labels > 2:
            print("\n" + "‚ö†Ô∏è" * 40)
            print("C·∫¢NH B√ÅO: DATASET C·∫¶N CLEAN!")
            print("‚ö†Ô∏è" * 40)
            print(f"\n   Dataset c√≥ {unique_labels} labels kh√°c nhau")
            print("   Paper ch·ªâ s·ª≠ d·ª•ng 2 labels: 0 (Benign) v√† 1 (Malicious)")
            print(f"\nüí° GI·∫¢I PH√ÅP:")
            print(f"   Ch·∫°y l·ªánh sau ƒë·ªÉ clean dataset:")
            print(f"   python clean_data.py")
            print(f"\n   Sau ƒë√≥ ch·∫°y l·∫°i script n√†y")
            print("\n   Ho·∫∑c th√™m d√≤ng sau v√†o code c·ªßa b·∫°n:")
            print("   df = df[df['Label'].isin([0, 1])]")
            print("\n" + "‚ö†Ô∏è" * 40)
    
    label_counts = analyze_labels(df)

    # Only continue if we have binary classification
    if 'Label' not in df.columns or df['Label'].nunique() != 2:
        print("\n‚ö†Ô∏è  Ph√¢n t√≠ch b·ªã d·ª´ng do dataset c·∫ßn clean")
        print("   Vui l√≤ng ch·∫°y: python clean_data.py")
        return

    # Run all analyses (console output only)
    df = analyze_queries(df)
    attack_counts, n_malicious = detect_attack_types(df)
    benign_counter, malicious_counter = analyze_vocabulary(df)
    analyze_special_characters(df)
    show_samples(df, n_samples=5)

    # ====================================================================
    # SUMMARY
    # ====================================================================
    print("\n" + "=" * 80)
    print("‚úÖ HO√ÄN TH√ÄNH PH√ÇN T√çCH")
    print("=" * 80)


if __name__ == "__main__":
    main()
