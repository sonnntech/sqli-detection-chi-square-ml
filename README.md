# ğŸ›¡ï¸ SQL Injection Detection using Chi-Square Feature Selection & Machine Learning

TÃ¡i hiá»‡n thÃ­ nghiá»‡m tá»« bÃ i bÃ¡o:

> **"Enhanced SQL injection detection using chi-square feature selection and machine learning classifiers"**  
> Emanuel Casmiry, Neema Mduma, Ramadhani Sinde (2025)

**Káº¿t quáº£ chÃ­nh:** Decision Tree Ä‘áº¡t **99.73% accuracy** sau khi Ã¡p dá»¥ng Chi-square feature selection (giáº£m 95% features: 49,607 â†’ 2,551)

---

## ğŸ“Œ TÃ­nh nÄƒng chÃ­nh

- âœ… Chi-square Feature Selection tá»± Ä‘á»™ng
- âœ… 5 Machine Learning Classifiers (DT, MNB, SVM, LR, KNN)
- âœ… Data Analysis Tools (7 sections phÃ¢n tÃ­ch)
- âœ… Stratified 5-Fold Cross Validation
- âœ… Visualization Ä‘áº§y Ä‘á»§ (8 charts)

---

## ğŸš€ Quick Start (3 bÆ°á»›c)

```bash
# 1. LÃ m sáº¡ch dataset
python clean_data.py

# 2. PhÃ¢n tÃ­ch dataset
python data_analysis.py

# 3. Cháº¡y thá»±c nghiá»‡m
python main.py
```

**Káº¿t quáº£:** 8 charts (PNG) + metrics in console + cleaned dataset

---

## ğŸ“‚ Cáº¥u trÃºc thÆ° má»¥c

```
.
â”œâ”€â”€ main.py                    # Main experiment
â”œâ”€â”€ clean_data.py              # Data cleaning
â”œâ”€â”€ data_analysis.py           # Data analysis (7 sections)
â”œâ”€â”€ requirements.txt
â””â”€â”€ data/
    â”œâ”€â”€ SQLiV3.csv            # Original (Kaggle)
    â””â”€â”€ SQLiV3_cleaned.csv    # Cleaned (auto-generated)
```

---

## ğŸ› ï¸ CÃ i Ä‘áº·t

```bash
# Táº¡o virtual environment
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt

# Download NLTK stopwords
python -c "import nltk; nltk.download('stopwords')"
```

---

## ğŸ—ƒï¸ Dataset

### Nguá»“n dá»¯ liá»‡u

**SQLiV3.csv** tá»« [Kaggle](https://www.kaggle.com/datasets/syedsaqlainhussain/sql-injection-dataset)

| Sentence       | Label |
|----------------|-------|
| SQL query text | 0/1   |

- `0` = Normal query (Benign)
- `1` = SQL Injection (Malicious)

### âš ï¸ Váº¥n Ä‘á» vÃ  Giáº£i phÃ¡p

**Váº¥n Ä‘á»:** Dataset gá»‘c cÃ³ thá»ƒ chá»©a:
- 42 labels thay vÃ¬ 2
- Missing values, duplicates
- Imbalanced data (ratio 19268:1)

**Giáº£i phÃ¡p:** Cháº¡y `clean_data.py` trÆ°á»›c
- âœ… Filter chá»‰ giá»¯ label 0 vÃ  1
- âœ… XÃ³a duplicates, missing values
- âœ… Balance data â†’ ~30,000 samples (50-50)

---

## ğŸ“Š PhÃ¢n tÃ­ch Dataset

```bash
python data_analysis.py
```

### PHáº¦N 1: Kháº£o sÃ¡t cÆ¡ báº£n Dataset

**KÃ­ch thÆ°á»›c dataset:**
- Sá»‘ dÃ²ng (samples): 30,405
- Sá»‘ cá»™t (features): 2

**CÃ¡c cá»™t trong dataset:**

| # | Column   | Non-Null Count | Dtype  |
|---|----------|----------------|--------|
| 0 | Sentence | 30,405 non-null | object |
| 1 | Label    | 30,405 non-null | int64  |

- Memory usage: ~3 MB
- Missing values: KhÃ´ng cÃ³ missing values

**5 dÃ²ng Ä‘áº§u tiÃªn:**

| | Sentence | Label |
|---|----------|-------|
| 0 | `" or pg_sleep  (  __TIME__  )  --` | 1 |
| 1 | `AND 1 = utl_inaddr.get_host_address ( ...` | 1 |
| 2 | `select * from users where id = '1' or @@1 ...` | 1 |
| 3 | `select * from users where id = 1 or 1#" ( ...` | 1 |
| 4 | `select name from syscolumns where id = ...` | 1 |

### Key Insights

| Category | Metric | Detail |
|----------|--------|--------|
| **Dataset** | Total samples | ~30,000 queries (balanced 50-50) |
| | Avg length (Benign) | 80 chars |
| | Avg length (Malicious) | 150 chars (2x longer) |
| **Attack Types** | Comment-based | 70% â€” `--`, `#`, `/* */` |
| | Boolean-based | 60% â€” `OR 1=1`, `AND 1=1` |
| | UNION-based | 40% â€” `UNION SELECT` |
| | Time-based | 15% â€” `SLEEP()`, `WAITFOR` |
| | Error-based | 11% â€” `CAST`, `CONVERT` |
| | Stacked queries | 8% â€” `;` multiple statements |
| **Top Words** | Benign | `select`, `from`, `where`, `id` |
| | Malicious | `union`, `sleep`, `or`, `and`, `convert` |

---

## ğŸ§  Experiment Workflow (`main_imporoved.py`)

```bash
python main_imporoved.py
```

### Step 1: Load & Preprocess

- Loaded **30,405** samples (Benign: 19,128 â€” 62.91%, Malicious: 11,277 â€” 37.09%)
- Text cleaning: lowercase, giá»¯ SQL-specific tokens (`'`, `--`, `;`, `|`)
- Remove stop words (giá»¯ SQL keywords: `select`, `or`, `and`, `union`...)

### Step 2: Train-Test Split

| | Size |
|---|---|
| Train | 24,324 (80%) |
| Test | 6,081 (20%) |

### Step 3: TF-IDF Vectorization

- Vocabulary size: **20,844**
- Train shape: (24,324 x 20,844)

### Step 4: Chi-Square Feature Selection

- Optimal k = **2,551** (from paper)
- Features reduced: 20,844 â†’ 2,551 (**87.8% reduction**)

### Step 5: Cross-Validation Results (Stratified 5-Fold)

| Model | Before FS | After FS | Change |
|-------|-----------|----------|--------|
| MNB | 94.64% Â± 0.18% | 93.56% Â± 0.08% | -1.08% |
| LR | 94.10% Â± 0.07% | 94.17% Â± 0.12% | +0.07% |
| **DT** | 78.91% Â± 0.25% | **99.51% Â± 0.11%** â­ | **+20.60%** |
| SVM | 97.45% Â± 0.17% | 97.84% Â± 0.18% | +0.39% |
| KNN | 49.47% Â± 0.43% | 91.48% Â± 0.48% | +42.01% |

### Step 6: Test Set Evaluation (Best Model: Decision Tree)

| Metric | Score |
|--------|-------|
| Accuracy | **98.37%** |
| Precision | 99.40% |
| Recall | 96.19% |
| F1-Score | 97.77% |
| FPR | 0.34% |
| Misclassification | 1.63% |

### Step 7: Computational Efficiency (Decision Tree)

| Metric | Before FS | After FS | Improvement |
|--------|-----------|----------|-------------|
| Training Time | 1.6921s | 0.1213s | **93% faster** |
| Inference Time | 0.0020 ms/query | 0.0001 ms/query | **95% faster** |
| Model Size | 0.79 MB | 0.03 MB | **96% smaller** |

---

## ğŸ“ˆ Visualization

Output: `results/`

| File | MÃ´ táº£ |
|------|--------|
| `comparison.png` | Grouped bar chart â€” Before vs After FS |
| `tsne_comparison.png` | t-SNE 2D â€” Before vs After FS |

![Model Comparison](results/comparison.png)
*Before vs After Feature Selection*

![t-SNE](results/tsne_comparison.png)
*t-SNE: Before FS (overlap) vs After FS (tÃ¡ch biá»‡t rÃµ rÃ ng)*

---

## ğŸ”§ Troubleshooting

**ValueError: shape mismatch**
```bash
python clean_data.py  # Cháº¡y trÆ°á»›c khi analysis
```

**ModuleNotFoundError**
```bash
pip install -r requirements.txt
```

**NLTK stopwords not found**
```bash
python -c "import nltk; nltk.download('stopwords')"
```

---

## ğŸ§ª Táº¡o Dataset báº±ng SQLMap (Optional)

Theo phÆ°Æ¡ng phÃ¡p cá»§a bÃ i bÃ¡o - táº¡o dataset tá»« SQLMap:

```bash
# 1. Cháº¡y DVWA
docker run -d --name dvwa -p 8080:80 vulnerables/web-dvwa:1.9

# 2. Generate payloads
sqlmap -u "http://localhost:8080/vulnerabilities/sqli/?id=1&Submit=Submit" \
  --batch --level=2 --risk=1 --technique=BEU -v 3 --stop=50 > sqli_payloads.txt

# 3. Táº¡o normal.txt vá»›i input bÃ¬nh thÆ°á»ng
echo -e "id=1\nid=2\nid=admin" > normal.txt

# 4. Cháº¡y build_dataset.py Ä‘á»ƒ merge
python build_dataset.py
```

**Chi tiáº¿t:** Xem [QUICK_START_GUIDE.md](QUICK_START_GUIDE.md)

---

## ğŸ“ Tham kháº£o

**Paper:**
Casmiry, E., Mduma, N., & Sinde, R. (2025). *Enhanced SQL injection detection using chi-square feature selection and machine learning classifiers.* Frontiers in Big Data. [DOI:10.3389/fdata.2025.1686479](https://doi.org/10.3389/fdata.2025.1686479)

**Dataset:**
SQLiV3 - [Kaggle](https://www.kaggle.com/datasets/syedsaqlainhussain/sql-injection-dataset)

---

## âœï¸ Má»Ÿ rá»™ng

Ã tÆ°á»Ÿng phÃ¡t triá»ƒn:
- [ ] Implement coarse + fine search tá»± Ä‘á»™ng
- [ ] So sÃ¡nh Chi-square vs IG vs MI
- [ ] ThÃªm Random Forest / XGBoost
- [ ] Deep Learning (LSTM, BERT)
- [ ] Confusion matrix, ROC curves
- [ ] Test trÃªn external datasets